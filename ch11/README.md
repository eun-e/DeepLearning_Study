## 11-1. 강화 학습의 기본 개념

#### 🔍 개념 정리
- 강화학습: 에이전트가 환경과 상호 작용하며 보상을 최대화하는 행동 정책을 학습하는 방법
  - 에이전트는 매 시점에서 환경의 상태(state)를 관찰하고 행동을 선택한 뒤 그 결과로 보상을 받으며 다음 상태로 전이됨
  - 목표: 누적 보상을 극대화하는 정책 찾기
 
- 구성요소
  1. 에이전트: 환경 내에서 행동을 선택하는 주체
  2. 환경: 에이전트가 상호 작용하는 외부 세계로, 상태와 보상을 제공함
  3. 보상: 긍정적인 보상은 좋은 행동을, 부정적인 보상은 나쁜 행동을 강화함
  4. 정책: 에이전트가 상태를 관찰하고 행동을 선택하는 전략, 확률 분포로 표현되며 특정 상태에서 특정 행동을 선택할 확률

````python
def choose_action(self, stat e): 
  # 현재 상태에서 최선의 행동 선택
  import random

  # 90% 확률로 최선의 행동, 10% 확률로 랜덤 탐험
  # random.random()은 0 이상 1 미만의 실수를 랜덤으로 반환함
  if random.random() < 0.9:    
    # 막대가 오른쪽으로 기울면 오른쪽으로 이동 
    return 1 if state['pole_angle'] > 0 else -1 
  else: 
    return random .choice([-1 , 1]) # 탐험
````

- 가치 함수: 특정 상태에서 미래에 받을 누적 보상의 기대치를 나타냄
  - "지금 이 상태에 있으면 앞으로 평균적으로 얼마나 더 벌 수 있는지"
- Q-함수: 특정 상태에서 특정 행동을 취했을 때 누적 보상 기대치를 나타내며, Q(s,a)로 표현됨
  - "지금 이 상태에서 이 행동을 하면 앞으로 평균적으로 얼마나 더 벌 수 있는지:
        
<br>

## 11-2. DQN(Deep Q-Network) 구현

#### 🔍 개념  
- Q-Learning: 에이전트가 상태 s와 행동a에 대한 Q-값을 업데이트하며 최적 정책을 학습하는 방법
- DQN: Q-Learning을 심층 신경망으로 확장하여 복잡한 상태 공간에서도 Q-함수를 근사함
  - 경험 리플레이: 에이전트 경험을 저장해 무작위로 샘플링해 학습함으로써 데이터 상관관계를 줄임
  - 타깃 네트워크: Q-값 업데이트의 안정성을 위해 일정 주기마다 업데이트되는 별도 네트워크를 사용   

#### ❓ 추가 개념 정리
<<Q-Learning 업데이트 식>>
<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/4a4f148c-1b64-4d23-8297-18b944da809f" />
- 현재 Q(s,a)를 target 쪽으로 조금씩 끌어당김

<<DQN>>









