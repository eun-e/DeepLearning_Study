## 11-1. 강화 학습의 기본 개념

#### 🔍 개념 정리
- 강화학습: 에이전트가 환경과 상호 작용하며 보상을 최대화하는 행동 정책을 학습하는 방법
  - 에이전트는 매 시점에서 환경의 상태(state)를 관찰하고 행동을 선택한 뒤 그 결과로 보상을 받으며 다음 상태로 전이됨
  - 목표: 누적 보상을 극대화하는 정책 찾기
 
- 구성요소
  1. 에이전트: 환경 내에서 행동을 선택하는 주체
  2. 환경: 에이전트가 상호 작용하는 외부 세계로, 상태와 보상을 제공함
  3. 보상: 긍정적인 보상은 좋은 행동을, 부정적인 보상은 나쁜 행동을 강화함
  4. 정책: 에이전트가 상태를 관찰하고 행동을 선택하는 전략, 확률 분포로 표현되며 특정 상태에서 특정 행동을 선택할 확률

````python
def choose_action(self, stat e): 
  # 현재 상태에서 최선의 행동 선택
  import random

  # 90% 확률로 최선의 행동, 10% 확률로 랜덤 탐험
  # random.random()은 0 이상 1 미만의 실수를 랜덤으로 반환함
  if random.random() < 0.9:    
    # 막대가 오른쪽으로 기울면 오른쪽으로 이동 
    return 1 if state['pole_angle'] > 0 else -1 
  else: 
    return random .choice([-1 , 1]) # 탐험
````

- 가치 함수: 특정 상태에서 미래에 받을 누적 보상의 기대치를 나타냄
  - "지금 이 상태에 있으면 앞으로 평균적으로 얼마나 더 벌 수 있는지"
- Q-함수: 특정 상태에서 특정 행동을 취했을 때 누적 보상 기대치를 나타내며, Q(s,a)로 표현됨
  - "지금 이 상태에서 이 행동을 하면 앞으로 평균적으로 얼마나 더 벌 수 있는지:
        
<br>

## 11-2. DQN(Deep Q-Network) 구현

#### 🔍 개념  
- Q-Learning: 에이전트가 상태 s와 행동a에 대한 Q-값을 업데이트하며 최적 정책을 학습하는 방법
- DQN: Q-Learning을 심층 신경망으로 확장하여 복잡한 상태 공간에서도 Q-함수를 근사함
  - 경험 리플레이: 에이전트 경험을 저장해 무작위로 샘플링해 학습함으로써 데이터 상관관계를 줄임
  - 타깃 네트워크: Q-값 업데이트의 안정성을 위해 일정 주기마다 업데이트되는 별도 네트워크를 사용   

#### ❓ 추가 개념 정리
**<Q-Learning 업데이트 식>** <br>
<img width="350" height="75" alt="image" src="https://github.com/user-attachments/assets/4a4f148c-1b64-4d23-8297-18b944da809f" />
- 현재 Q(s,a)를 target 쪽으로 조금씩 끌어당김

**<DQN>**<br>
- 경험 리플레이: (s,a,r,s') 경험을 저장해두고 랜덤 샘플링해서 학습함
  - (s,a,r,s') 이런 데이터를 바로 학습하면 문제 발생 -> 데이터가 시간 순서대로 강하게 연결되고 발산 가능
  - 메모리에 다 저장해두었다가 랜덤으로 꺼내서 학습함
  - 샘플링: 저장해둔 것 중에서 랜덤으로 뽑는 것
  - Replay Buffer가 어느 정도 쌓일 때까지 기다림
- learn 함수: 타깃 네트워크와의 차이를 줄이는 방식으로 MSE 손실을 계산함
- 타깃 네트워크: target 계산에 쓰는 네트워크를 따로 둬서 계산함
  - 소프트 업데이트(tau)를 통해 주기적으로 업데이트됨
  - 학습용 네트워크와 타킷 네트워크 두 개를 따로 둠
````
⭐ 타깃 네트워크 vs 학습용 네트워크

우리는 prediction -> target 으로 맞추려고 학습함 but target도 계속 변함
Target=r+γmaxQ(s′,a′) : target을 계산할 때 이용하는 Q도 어차피 지금 학습 중인 신경망
-> 학습용 네트워크와 타킷 네트워크를 따로 둬서 학습용은 예측값 계산, 타깃은 목표값 계산을 함

Target=r+γmaxQ_target​(s′,a′)
-> 타깃 네트워크는 자주 안 변하고(소프트 업데이트) 비교적 고정됨
````

<br>

## 11-3. 정책 그래디언트 기법 적용

#### 🔍 개념 정리
- 직접 파라미터화하고, 그 파라미터를 조정해 보상을 최대화하는 방식
  - 정책 π(a|s): 에이전트가 상태 s에 대해 행동 a를 선택할 확률 분포
- REINFORCE 알고리즘: 에피소드 단위로 경험을 수집한 후 각 행동에 대해 보상의 기댓값을 게산하여 정책의 파라미터를 업데이트함
- Policy Network: 상태를 입력받아 각 행동의 선택 확률을 출력하는 간단한 신경망
  - 아예 행동을 뽑는 확률 분포 자체를 신경망이 출력함
  - Ex. CartPole - 네트워크 출력[p(left), p(right)]을 softmax 확률로 만듦
- selection_action 함수: 현재 상태에서 정책 네트워크의 확률 분포를 기반으로 행동을 샘플링하고 로그 확률 반환
  - 보상을 적게 받은 행동이면 그 행동의 확률을 낮추고 아니면 반대로 함
- reinforce_update 함수: 에피소드에서 얻은 보상들을 누적해 정규화한 후 reinforce 알고리즘에 따라 정책 네트워크의 파라미터를 업데이트함
  - 매 에피소드마다 상태, 행동, 보상 등을 저장함
  - 에피소드가 끝나면 각 시점의 미래 누적 보상 G_t를 계산함


<br>

## 11-5. 모델 평가

#### 🔍 개념 정리
- 평가 지표
  - 누적 보상: 에피소드 돋안 에이전트가 받은 총 보상
  - 평균 에피소드 길이: 에이전트가 얼마나 오래 버티는지를 나타내며 안정성의 척도가 될 수 있음
  - 성공률: 특정 목표의 달성 비율

- 성능 개선 기법
  - e-greedy 탐색: 에이전트가 탐험과 이용 사이 균형을 맞추도록 e 값을 점차 줄임
    - 확률 e로 랜덤 행동을 하고(탐험), 확률 1-e로 최선의 행동을 함 
  - 경험 리플레이: 에이전트의 경험을 저장해 무작위 샘플링으로 학습함으로서 데이터 상관성을 줄이고 안정적인 학습을유도함
    - 연속도니 경험은 비슷해서 학습이 불안정해짐(상관성이 큼)
    - 버퍼가 어느정도 쌓일 때까지는 학습을 안 하거나 warm-up(랜덤 행동으로 채움)을 함
  - 타깃 네트워크: DQN에서 사용하는 기법으로, 주기적으로 타깃 네트워크를 업데이트하여 학습의 안정성을 높임
    - θ값을 가끔씩만 복사해서 업데이트함
  - 정책 그래디언트 기법: 직접 업데이트하는 기법을 활용해 직접적인 보상 극대화를 도모함
