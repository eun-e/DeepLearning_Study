##  10-1. 오디오 신호 처리 기초

#### 🔍 개념 정리
- 샘플링: 연속적인 아날로그 신호를 일정한 시간 간격으로 측정해 이산적인 값으로 만드는 과정
  - 샘플링 레이트: 1초당 샘플 수, 단위는 Hz (사람이 들을 수 잇는 주파수: 약 20~20kHz)
  - 나이키스트 정리: 원본 신호 최대 주파수의 2배 이상으로 샘플링 필요
  - 샘플링이 클수록 시간적으로 더 부드러운 소리, 고주파 표현에 유리함
- 양자화: 각 샘플의 진폭을 유한한 수치로 표현하는 과정
  - 양자화 비트: 각 샘플의 진폭을 몇 비트로 표현할지 결정 (16 or 24비트 사용)
- 디지털 오디오 변환 과정
  1. 아날로그 신호: 연속적인 시간과 진폭을 가진 원본 신호, 자연계에서 발생하는 실제 소리
  2. 샘플링: 일정한 시간 간격으로 신호 값
  3. 양자화: 연속적인 진폭값을 유한한 개수의 이산적 값으로 변환, 양자화 비트 수가 클수록 정밀한 표현 가능
- 오디오 신호
  - 진폭: 신호의 크기로 소리의 세기를 나타냄(볼륨)
  - 주파수: 1초당 진동 횟수로 소리의 높낮이를 결정함(Hz)
  - 위상: 신호의 시작점에 대한 상대적인 위치로 소리의 공간적 특성에 영향을 줌
  - 음색: 같은 음높이와 세기를 가진 소리를 구별하게 하는 특성으로 배음 구조에 의해 결정됨
- PCM: 아날로그 신호를 디지털로 변환하는 가장 기본적인 방식
- WAV: PCM 데이터를 저장하는 표준 파일 형식(scipy.io.wavfile/librosa library in python)
   - RIFF 헤더: 파일의 시작을 나타내며 파일 크기와 포맷 정보를 포함함
   - fmt 청크: 포맷 정보를 포함하며 샘플링 레이트, 비트 깊이, 채널 수 등을 명시함
   - data 청크: 실제 오디오 데이터를 포함함
     
````python
import librosa
import numpy as np
from IPython.display import Audio, display # 오디오를 불러와 화면에 출력
````

- 샘플링 레이트 변환: 오디오 데이터를 수집할 때 샘플링 레이트가 서로 다른 경우
  1. 오디오를 tensor로 변환: torch.tensor(audio).unsqueeze(0).float()
     - (N,) 1차원 형태의 audio.shape을 (1,N)로 변경함 -> 채널 차원 추가(channel=1) <br>
       ⎿ batch가 아니라 channel을 추가하는 것! batch는 DataLoader에 붙거나 모델 입력에 맞춰 추가로 unsqueeze함
     - 원래 int 였던 자료를 float으로 자료형 바꾸기 - Pytorch 연산하기 위해서
  2. torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)
     - 점 간격을 재조정해서 새 시간축에 맞게 값을 재계산함
     - 다운샘플링은 aliasing을 유발할 수 있음

- 노이즈 제거
  1. 저역 통과 필터: 낮은 주파수만 통과, 목소리만 남기고 잡음 제거 등
     - aliasing을 방지하기 위해 다운 샘플링 전에 저역 통과 필터를 적용함
  2. 고역 통과 필터: 저주파 노이즈 제거, 마이크 바람 소리 제거 등
  3. 대역 통과 필터 특정 주파수만 통과 가능하게 함

- 정규화: 오디오 신호의 볼륨 레벨을 일정하게 맞추기 위해
  1. 피크 정규화: 가장 큰 피크가 목표값(1)에 오도록 전체를 스케일링함
     - waveform=[0.5, 0.3, -0.2, 0.8]일 때 peak_normalized=[0.5, 0.3, -0.2, 0.8]/0.8
     - 결과적으로 최대값을 1로 조정
     - 디지털 오디오의 표현 범위가 [-1,1]이기 때문에 1에 맞추려고함
  2. RMS 정규화: 신호의 평균적인 크기를 나타내는 값이 목표 값에 오도록 스케일링함
     - 평균 에너지를 맞추는 것이기 때문에 평균 에너지가 1이 되면 거의 모든 샘플링이 1 근처 -> 클리핑 발생
     - 클리핑이란? 신호 값이 표현 가능한 최대 범위를 넘어가서 강제로 잘리는 현상

- ⭐ 푸리에 변환: 시간 영역의 신호를 주파수 영역으로 변환해 신호의 주파수 성분을 분석할 수 있게 함
  - 신호에 어떤 주파수들이 섞여 있는지 알려줌 but 시간 정보가 사라짐 -> STFT 등장 이유 
  - STFT: 오디오 신호를 작은 시간 창으로 나누고, 각 창에 대해 푸리에 변환을 적용해 시간에 따른 주파수 변화 분석
  - STFT의 결과를 스펙트로그램으로 부르며, 시간-주파수-진폭의 3차원 표현임

- MFCC: 인간의 청각 시스템이 주파수를 인식하는 방법을 모방한 특성
- Zero Crossing Rate: 신호가 영점을 통과하는 비율 -> 값이 크면 고주파 성분이 많다는 의미
  - 에너지는 신호의 강도를 나타냄
  - 묵음 구간과 발성 구간을 구분하는 데 유용함
  - 음성 분석에서 무성음(s,f,ㅅ 같은 소리)는 ZCR이 높음 -> 성대 진동이 없고 공기 마찰 소리
  - 유성음(ㅏ,ㅗ 같은 모음)은 ZCR이 낮음 -> 성대가 규칙적으로 진동해서 거의 주기적인 파형임

#### 📝 ZCR 코드 정리
````python
def compute_zcr(waveform, frame_length, hop_length): 
  # 프레임으로 분할 
  frames = waveform.unfold(l, frame_length, hop_length)

  #부호변화감지 
  signs = torch.sign(frames) 
  signs_diff = torch.abs(signs[:,:.1:] - signs[:,:,:-1]) / 2

  # ZCR 계산 
  zcr = torch.sum(signs_diff, dim=2) / (frame_length - 1) 
  return zcr
````
````text
waveform.shape = (1,10) 이면 오디오 1개, 샘플 10개라는 의미
waveform.unfold에서 1의 의미: dim=1 시간축(샘플수)을 따라 자른다는 말
waveform = [[ x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 ]]
   ↓
frame_length이 4라면 한 조각에 샘플 4개씩 담겠다는 뜻
[x0 x1 x2 x3]   ← frame 0
[x1 x2 x3 x4]   ← frame 1
[x2 x3 x4 x5]   ← frame 2
...
   ↓
hop_length = 2는 다음 프레임으로 갈 때 2칸 이동한다는 뜻
frame 0: x0 x1 x2 x3
frame 1:       x2 x3 x4 x5
frame 2:             x4 x5 x6 x7
frame 3:                   x6 x7 x8 x9
````
      
#### ❓추가 정리 사항
- 샘플링 레이트 변환 vs 정규화
  - 정규화는 값의 진폭(범위)을 맞추는 것
  - 샘플링 레이트는 1초에 몇개로 쪼개서 표현했냐를 나타내는 것<br>
    ⎿ 오디오 텐서의 shape 예시: 단일 오디오-(N,)/(1,N) vs 배치로 묶이면 (batch, channel, time)<br>
    ⎿ 샘플링 레이트가 shape을 결정하는 건 아니고 같은 길이일 때 time 축 길이를 결정함<br>
    ⎿ 샘플링 레이트가 다르면 같은 1초라도 time 길이와 주파수 표현 범위가 달라져서 통일함
- 보통 샘플링 레이트 변환은 다운샘플링 but 정보 손실 발생 가능
````text
WHY? 사람 음성 정보는 대부분 8kHz 이하여서 나이키스트 정리에 의해 16kHz 샘플링이면 충분
     음악이나 영상은 보통 40kHz가 넘기 때문에 다운샘플링 필요
````
- shape 정리
```text
▪️Channel이란? 하나의 데이터 안에 존재하는 병렬 신호 개수
   모노면 channel=1, 스테레오면 channel=2
▪️Batch란? 모델에 한 번에 넣는 데이터 개수

-> (batch, channel, time)=(32,1,16000)이면 32개 오디오, 각 오디오가 모노, 각 오디오가 16000 샘플이라는 뜻
````
- Aliasing: 고주파가 저주파로 잘못 보이는 현상 <br>
  ⎿ 우리는 빠르게 진동하는 걸 자주 보지 않으면 진동을 구별하지 못함 <br>
      ex. 분쇄기가 너무 빠르게 회전하면 멈춰있는 것처럼 보임 <br>
  ⎿ 최소 진동당 2번을 봐야함 -> 나이키스트 정리

<br>


## 10-2. 파이토치를 이용한 오디오 처리
- 오디오 모델 학습을 위해서는 데이터셋 구성이 필요함 -> Dataset, DataLoader 클래스 활용
  - Dataset: 데이터 하나를 어떻게 가져올지 정의하는 클래스 (데이터 꺼내는 방법 정의)
  - DataLoader: Dataset에서 데이터를 꺼내서 batch로 묶어주는 도구
    ````python
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    ````
    
- 배치 처리 설계
  - 패딩: 배치 내 모든 오디오를 가장 긴 오디오 길이에 맞춰 패딩함
  - 자르기: 모든 오디오를 동일한 길이로 자름
  - 동적 배치 크기: 비슷한 길이의 오디오끼리 묶어 배치를 구성함
    
- 스펙트로그램: 오디오 신호의 시간-주파수 표현 <br>
<img width="644" height="335" alt="image" src="https://github.com/user-attachments/assets/9457f707-38b5-4fdb-9e1a-a76e9588b1e9" /> <br>
  - x축은 시간, y축은 주파수를 나타내며 색은 세기를 나타냄
  - 빨간색은 강한 주파수, 파란색은 약한 주파수
````python
spectrogram_transform = torchaudio.transforms .Spectrogram( 
  n_fft=1024,         # FFT 크기, 주파수 해상도 결정
  win_length=1024,    # 창 함수의 길이, 시간 해상도에 영향
  hop_length=512,     # 연속된 프레임 간의 간격, 시간 해상도 결정
  center=True,        # 창 함수 타입
  pad_mode="reflect", 
  power=2.0           # 2.0은 파워 스펙트로그램, 1.0은 진폭 스펙트로그램
  )
````
````text
FFT는 소리를 주파수로 바꿔주는 계산 방법 = 푸리에 변환을 빠르게 계산하는 알고리즘
☝️ "이 신호 안에 어떤 주파수가 얼마나 있는지"

1) n_fft는 한 번 FFT를 할 때 몇 개 샘플을 사용할지 결정 (몇 개의 점으로 주파수 축을 나눌지)
   -> n_fft를 늘리면 주파수를 더 촘촘하게 나눠서 분석
   주파수 해상도 = sample_rate/n_fft 여서 주파수 해상도도 결정함
   ex) 16000/256 = 62.5Hz vs 16000/1024 = 15.6Hz
       -> n_fft가 크면 계산량이 늘어나는 대신 주파수 해상도가 좋아짐

2) win_length는 한 window에 들어가는 샘플 수 (보통 n_fft랑 같은 값)
   STFT는 짧은 시간 구간만 잘라서 그 안에서 FFT를 하는 것 -> 잘라낸 한 덩어리가 window
   ex) waveform = [[ x0 x1 x2 x3 x4 x5 x6 x7 .. ]]일 때 window 길이가 4면 [x0 x1 x2 x3]이 window 하나
   window의 시간 길이 = win_length/sample_rate
   -> win_length가 클수록 한 번에 긴 구간 분석 가능, 시간 변화가 더 뭉개짐
   -> win_length가 작을수록 시간 해상도가 좋아짐
   프레임 간 시간 간격 = hop_length/sample_rate이므로 hop_length가 작을수록 더 자주 분석함
   -> hop_length가 작을수록 시간 해상도가 좋아짐

3) center는 각 프레임을 중앙 기준으로 정렬할지 결정

4) pad_mode는 padding을 어떻게 채울지 결정 (reflect는 반사해서 채움)
````

- MFCC
````python
mfcc_transform = torchaudio.transforms.MFCC( 
  sample_rate=sample_rate, 
  n_mfcc=13,             # 추출할 MFCC 계수의 수(일반적으로 13개)
  melkwargs={            
    'n_fft': 1024,   
    'n_mels': 40,        # Mel 필터 뱅크의 수
    'hop_length': 512,  
    'mel_scale': 'htk'
  }
)
````

#### ❓ 추가 정리 사항
````text
MFCC는 소리를 사람 귀에 가깝게 요약한 특징 벡터
-> 사람의 귀는 선형적이지 않아서 100Hz와 200Hz의 차이는 크게 느끼지만 10,000Hz와 11,000Hz의 차이는 잘 못 느낌
-> n_mfcc는 결과 벡터 차원이 13개라는 의미, 한 프레임이 13개의 숫자로 표현됨 

☝️ melwargs란? 멜 스펙트로그램을 만들 때 쓰는 설정들
[프레임 분할, FFT 수행, 멜 필터 적용, log 적용] 순서로 멜 스펙트로그램이 완성되고 DCT 수행을 통해 MFCC가 됨
-> n_mels: FFT 스펙트럼을 멜 스케일로 압축할 때 쓰는 멜 필터 개수
           한 프레임이 40차원의 멜 스펙트럼 벡터가 됨 

☝️ 멜 스펙트럼이란?
Mel Scale은 사람 귀가 느끼는 방식에 맞게 주파수를 재배치한 것으로 선형 주파수를 비선형 멜 단위로 바꿈
Ex. 0Hz, 1Hz, 2Hz, 3Hz, ... 8000Hz 원래 스펙트럼이 이런식으로 촘촘하게 생겼다면,
    멜 필터는 저음은 좁게 [0~100Hz], 고음은 넓게 [7000~8000Hz] 이런식으로 간격을 다르게 해서 40개 정도로 나눔
    -> n_mels = 40인 case
    ∑(스펙트럼 값 x 멜 필터 가중치)을 하면 최종 [ 5.1, 3.2, 0.8, 6.7, ... ]   ← 40개
    -> 필터 하나를 확대해서 보면 보통 삼각형 모양의 가중치를 가짐
       필터가 가장 관심 있어 하는 중심 주파수는 가중치가 크고 양 끝은 가중치가 작음 

log 적용해서 분포 안정화시키고 DCT통해 상관 관계를 제거해 저주파 성분만 남김 
````
<img width="380" height="200" alt="image" src="https://github.com/user-attachments/assets/9fd07b42-a015-4c17-9d5b-0d02e7b624fd" />
<br>
<br>

- 데이터 증강 기법: 모델 훈련 시 과적합을 방지함
  1. 시간 스트레칭: 오디오 속도 변경
  2. 피치 시프팅: 음높이 변경
  3. 볼륨 조절: 볼륨 증가/감소
  4. 노이즈 추가: 배경 노이즈 추가
  5. 주파수 마스킹: 특정 주파수 대역 차단
  6. 시간 마스킹: 특정 시간 구간 차단

<br>

## 10-3. WaveNet 모델 구현



