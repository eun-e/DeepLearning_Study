## Chapter 8-1. 컴퓨터 비전의 주요 과제

#### 🔍 개념 정리
- 전통적인 CV 기술은 특징 추출기에 크게 의존함 → 이런 특징들이 SVM이나 RF 분류기의 입력으로 활용됨
- 분야에 변화를 가져온 게 **"딥러닝"**
- 합성곱 신경망: 데이터로부터 특징을 자동으로 학습할 수 있는 능력을 가지고 있음 (특징 추출기 필요 ↓)
- 이미지 분류 vs 객체 탐지
  - 이미지 분류: 한 장의 이미지 전체를 보고, 그 이미지가 어떤 하나의 클래스(레이블)에 속하는지 맞힘
  - 객체 탐지: 이미지 내에 여러 객체가 있을 수 있음을 가정하고, 각 개체의 번주와 위치를 식별함
    1. 특징 추출기: 일반적으로 CNN 기반의 백본 네트워크
    2. 영역 제안 네트워크: 객체가 있을 만한 영역을 제안하는 네트워크
    3. 객체 분류기: 제안된 영역 내 객체를 분류하는 네트워크
    4. 경계 상자 회귀기: 객체의 정확한 위치와 크기를 조정하는 네트워크
    5. 후처리: 중복된 탐지 결과 제거 
- IOU: 객체 탐지 모델의 성능을 평가하는 핵심 지표 (0.5 이상이면 올바른 탐지로 간주)<br>
  <img width="450" height="45" alt="image" src="https://github.com/user-attachments/assets/957619ea-401e-49b0-9079-34c3fcd29348" /><br>
- mAP:모든 클래스에 대한 Average Precision 평균값으로 전반적인 성능을 평가함
- Segmentation: 이미지의 각 픽셀을 의미 있는 영역으로 분할하는 작업
  1. 의미론적 - 이미지의 각 픽셀을 미리 정의된 클래스로 분류함 
  2. 인스턴스 - 같은 클래스의 서로 다른 인스턴스를 구분함
<br>

#### ❓추가 정리 사항
1. 의미론적 segmentation
   - 픽셀마다 클래스만 구분
   - 같은 클래스면 모두 같은 색
   - 객체 구분 안 함
   ```` text
   사람 픽셀 → 전부 파란색
   자동차 픽셀 → 전부 노란색
   배경 픽셀 → 전부 회색
   
   but 사람 2명이 있어도 '사람'이라는 클래스 하나로만 처리
   ````
2. 인스턴스 segmentation
   - 픽셀 단위 분할 + 객체 구분
   - 같은 클래스라도 서로 다른 ID
   - 셀 수 있는 객체만 다룸!!! <br>
     → 도로, 하늘, 벽, 잔디, 배경 같은 것들은 인스턴스 X <br>
     → 객체 영역만 마스크가 있고 나머지 픽셀은 무시되거나 배경으로 통째로 처리됨
   ````text
   사람 1 픽셀 → 파란색
   사람 2 픽셀 → 초록색
   자동차 픽셀 → 노란색
   배경 픽셀 → 없음 or 회색
   ````
3. 파놉틱 segmentation
   - semantic + instance 합친 것
   - 모든 픽셀에 클래스 정보 + 인스턴스 ID
   - 인스턴스와 달리 도로, 하늘, 벽, 잔디 등도 다 의미론적 분할로 처리
<br>
cf. 마스크: 관심 있는 객체는 1로 아닌 픽셀은 0으로 두어 행렬 만들기
<br>

## Chapter 8-2. ResNet 구현과 활용

#### 🔍 개념 정리
- 전이 학습: 한 작업에서 학습된 지식을 다른 관련 작업에 적용하는 기법
<br>

## Chapter 8-3. 객체 탐지 모델 구현

#### 🔍 개념 정리
- 앵커 박스: 객체 탐지 모델에서 다양한 크기와 비율의 객체를 탐지하기 위해 사용하는 미리 정의된 경계 상자 템플릿<br>
  → 모델은 앵커 박스에 대한 상대적인 오프셋을 예측해 경계 상자를 미세 조정함
- 바운딩 박스 회귀: 모델이 앵커 박스를 조정해 실제 객체의 위치와 크기를 더 정확하게 맞추는 과정
- 비최대 억제: 객체 탐지 모델의 후처리 단계, 동일한 객체에 대해 중복된 탐지 결과를 제거함
````text
기본 알고리즘
1. 신뢰도 점수에 따라 모든 탐지 결과를 내림차순으로 정렬함
2. 가장 높은 신뢰도를 가진 탐지 결과를 선택하고 최종 결과 목록에 추가함
3. 선택된 탐지 결과와 나머지 탐지 결과들 간의 IoU를 계산함
4. IoU가 지정된 임계값보다 큰 모든 탐지 결과를 제거함
5. 남은 탐지 결과에 대해 2~4단계를 반복함
````
- YOLO: 단일 단계 객체 탐지 알고리즘으로 이미지를 한 번만 처리해 객체의 위치와 클래스를 동시에 예측함
  1. 단일 네트워크: 하나의 합성곱 네트워크가 바운딩 박스와 클래스 확률을 동시에 예측함
  2. 그리드 기반 접근: 이미지를 SXS 그리드로 나누고, 각 그리드 셀이 객체의 중심을 포함하면 해당 셀이 객체 탐지를 담당함
  3. 앵커 박스: 각 그리드 셀은 B개의 바운딩 박스와 각 박스에 대한 신뢰도 점수를 예측함
  4. 클래스 예측: 각 그리드 셀은 C개의 클래스에 대한 조건부 확률을 예측
<br>

#### ❓추가 정리 사항
- 바운딩 박스: 실제로 물체를 감싸는 사각형
- 앵커 박스: '이 위치엔 이런 크기의 물체가 있을 수도 있다'라고 미리 가정해 두는 상자들
  - 이미지 전체에 격자처럼 미리 배치
  - 크기/비율이 여러 개
  - 모델이 '이 앵커를 조금 오른쪽으로, 조금 작게 하면 정답이다' 같이 보정함(offset 문제)
- 앵커 박스 + offset 예측 = 바운딩 박스
- anchors = [x_center, y_center, width, height]
  - anchors[:,0] → 앵커 중심 x
  - anchors[:,1] → 앵커 중심 y
  - anchors[:,2] → 앵커 width
  - anchors[:,3] → 앵커 height
````python
def encode_boxes(anchors, gt_boxes): 
# 앵커 박스에 대한 실제 바운딩 박스의 오프셋 계산
  # 정답 박스 중심이 앵커 중심에서 width의 몇 배만큼 이동했나
  tx = (gt_boxes[:, 0] - anchors[:, 0]) / anchors[:, 2]
  # 정답 박스 중심이 앵커 중심에서 height의 몇 배만큼 이동했나
  ty = (gt_boxes[:, 1] - anchors[:, 1]) / anchors[:, 3] 
  tw = torch.log(gt_boxes[:, 2] / anchors[:, 2]) 
  th = torch.log(gt_boxes[:, 3] / anchors[:, 3]) 
  return torch.stack([tx, ty, tw, th] , dim=1) # return 값이 offset임

def decode_boxes(anchors, offsets): 
  # 예측된 오프셋을 사용하여 앵커 박스 변환 
  x = offsets[:, 0] * anchors[:, 2] + anchors[:, 0] 
  y = offsets[:, 1] * anchors[:, 3] + anchors[:, 1] 
  w = torch.exp(offsets[:, 2]) * anchors[ , 2]
  h = torch.exp(offsets[:, 3]) * anchors[:, 3] 
  return torch.stack([x, y, w, h] , dim=1)
````
<br>

## Chapter 8-4. 세그먼테이션 실습

#### 🔍 개념 정리
- U-Net: 의료 이미지 세그먼테이션을 위해 개발된 완전 합성곱 네트워크의 변형
  - 인코더-디코더 구조: 인코더는 이미지의 문맥 정보를 포착하고 디코더는 정확한 지역화를 수행함
  - skip connection: 특정 맵을 해당하는 디코더 층으로 연결하여 공간 정보를 보존함
  - 대칭적 구조: 인코더와 디코더가 거의 대칭적인 구조를 가짐
- 손실함수 종류
  1. Binary cross-entropy: 각 픽셀을 독립적으로 분류 but 클래스 불균형에 약함
  2. Dice 손실: 예측 마스크와 실제 마스크 간의 겹침 정도를 측정
  3. 결합 손실: BCE와 Dice 손실을 결합해 각각의 장점을 활용함
- 모델 평가 지표
  1. 픽셀 정확도: 올바르게 분류된 픽셀의 비율
  2. IoU: 예측 마스크와 실제 마스크의 교집합을 합집합으로 나눈 값
  3. Dice 계수: IoU와 관련됨

#### ❓ 추가 정리 사항
- 트랜스포머의 인코더-디코더와 달리 U-Net에서는 정보를 압축했다가 복원하는 용도
  - 인코더: 입력을 의미 있는 표현으로 변환(차원 줄이고 의미 정보 확대)
    - CNN+max pooling: 저수준 → 고수준 특징 추출
  - 디코더: 그 표현을 이용해 원래 형태로 복원하거나 새로운 출력 생성
    - upsampling: 해상도 복원


<br>

## Chapter 8-5. 고급 응용 기법

#### 🔍 개념 정리
- 멀티태스크 학습: 하나의 모델로 여러 관련 작업을 동시에 학습하는 접근 방식
- 앙상블 기법: 여러 모델의 예측 결과를 결합해 단일 모델보다 더 나은 성능을 얻는 방법
  1. 평균 앙상블: 모든 모델의 예측을 단순히 평균화함
  2. 가중 평균 앙상블: 각 모델에 다른 가중치를 부여해 평균화함
  3. 최대 투표: 분류 작업에서 각 모델이 예측한 클래스 중 가장 많이 선택된 클래스를 최종예측으로 선택함
  4. 스태킹: 기본 모델의 예측을 입력으로 사용하는 메타 모델을 학습시킴
- 테스트 시간증강: 추론 단게에서 입력 이미지에 여러 변환을 적용하고, 각 변환된 이미지에 대한 예측 결과를 결합해 최종 예측의 정확도를 향상시키는 기법
    
