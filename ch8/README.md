## Chapter 8-1. 컴퓨터 비전의 주요 과제

#### 🔍 개념 정리
- 전통적인 CV 기술은 특징 추출기에 크게 의존함 → 이런 특징들이 SVM이나 RF 분류기의 입력으로 활용됨
- 분야에 변화를 가져온 게 **"딥러닝"**
- 합성곱 신경망: 데이터로부터 특징을 자동으로 학습할 수 있는 능력을 가지고 있음 (특징 추출기 필요 ↓)
- 이미지 분류 vs 객체 탐지
  - 이미지 분류: 한 장의 이미지 전체를 보고, 그 이미지가 어떤 하나의 클래스(레이블)에 속하는지 맞힘
  - 객체 탐지: 이미지 내에 여러 객체가 있을 수 있음을 가정하고, 각 개체의 번주와 위치를 식별함
    1. 특징 추출기: 일반적으로 CNN 기반의 백본 네트워크
    2. 영역 제안 네트워크: 객체가 있을 만한 영역을 제안하는 네트워크
    3. 객체 분류기: 제안된 영역 내 객체를 분류하는 네트워크
    4. 경계 상자 회귀기: 객체의 정확한 위치와 크기를 조정하는 네트워크
    5. 후처리: 중복된 탐지 결과 제거 
- IOU: 객체 탐지 모델의 성능을 평가하는 핵심 지표 (0.5 이상이면 올바른 탐지로 간주)<br>
  <img width="450" height="45" alt="image" src="https://github.com/user-attachments/assets/957619ea-401e-49b0-9079-34c3fcd29348" /><br>
- mAP:모든 클래스에 대한 Average Precision 평균값으로 전반적인 성능을 평가함
- Segmentation: 이미지의 각 픽셀을 의미 있는 영역으로 분할하는 작업
  1. 의미론적 - 이미지의 각 픽셀을 미리 정의된 클래스로 분류함 
  2. 인스턴스 - 같은 클래스의 서로 다른 인스턴스를 구분함
<br>

#### ❓추가 정리 사항
1. 의미론적 segmentation
   - 픽셀마다 클래스만 구분
   - 같은 클래스면 모두 같은 색
   - 객체 구분 안 함
   ```` text
   사람 픽셀 → 전부 파란색
   자동차 픽셀 → 전부 노란색
   배경 픽셀 → 전부 회색
   
   but 사람 2명이 있어도 '사람'이라는 클래스 하나로만 처리
   ````
2. 인스턴스 segmentation
   - 픽셀 단위 분할 + 객체 구분
   - 같은 클래스라도 서로 다른 ID
   - 셀 수 있는 객체만 다룸!!! <br>
     → 도로, 하늘, 벽, 잔디, 배경 같은 것들은 인스턴스 X <br>
     → 객체 영역만 마스크가 있고 나머지 픽셀은 무시되거나 배경으로 통째로 처리됨
   ````text
   사람 1 픽셀 → 파란색
   사람 2 픽셀 → 초록색
   자동차 픽셀 → 노란색
   배경 픽셀 → 없음 or 회색
   ````
3. 파놉틱 segmentation
   - semantic + instance 합친 것
   - 모든 픽셀에 클래스 정보 + 인스턴스 ID
   - 인스턴스와 달리 도로, 하늘, 벽, 잔디 등도 다 의미론적 분할로 처리
<br>
cf. 마스크: 관심 있는 객체는 1로 아닌 픽셀은 0으로 두어 행렬 만들기
<br>

## Chapter 8-2. ResNet 구현과 활용

#### 🔍 개념 정리
- 전이 학습: 한 작업에서 학습된 지식을 다른 관련 작업에 적용하는 기법
<br>

## Chapter 8-3. 객체 탐지 모델 구현

#### 🔍 개념 정리
- 앵커 박스: 객체 탐지 모델에서 다양한 크기와 비율의 객체를 탐지하기 위해 사용하는 미리 정의된 경계 상자 템플릿<br>
  → 모델은 앵커 박스에 대한 상대적인 오프셋을 예측해 경계 상자를 미세 조정함
- 바운딩 박스 회귀: 모델이 앵커 박스를 조정해 실제 객체의 위치와 크기를 더 정확하게 맞추는 과정
- 비최대 억제: 객체 탐지 모델의 후처리 단계, 동일한 객체에 대해 중복된 탐지 결과를 제거함
````text
기본 알고리즘
1. 신뢰도 점수에 따라 모든 탐지 결과를 내림차순으로 정렬함
2. 가장 높은 신뢰도를 가진 탐지 결과를 선택하고 최종 결과 목록에 추가함
3. 선택된 탐지 결과와 나머지 탐지 결과들 간의 IoU를 계산함
4. IoU가 지정된 임계값보다 큰 모든 탐지 결과를 제거함
5. 남은 탐지 결과에 대해 2~4단계를 반복함
````
- YOLO: 단일 단계 객체 탐지 알고리즘으로 이미지를 한 번만 처리해 객체의 위치와 클래스를 동시에 예측함
  1. 단일 네트워크: 하나의 합성곱 네트워크가 바운딩 박스와 클래스 확률을 동시에 예측함
  2. 그리드 기반 접근: 이미지를 SXS 그리드로 나누고, 각 그리드 셀이 객체의 중심을 포함하면 해당 셀이 객체 탐지를 담당함
<br>

#### ❓추가 정리 사항
- 바운딩 박스: 실제로 물체를 감싸는 사각형
- 앵커 박스: '이 위치엔 이런 크기의 물체가 있을 수도 있다'라고 미리 가정해 두는 상자들
  - 이미지 전체에 격자처럼 미리 배치
  - 크기/비율이 여러 개
  - 모델이 '이 앵커를 조금 오른쪽으로, 조금 작게 하면 정답이다' 같이 보정함(offset 문제)
- 앵커 박스 + offset 예측 = 바운딩 박스
- anchors = [x_center, y_center, width, height]
  - anchors[:,0] → 앵커 중심 x
  - anchors[:,1] → 앵커 중심 y
  - anchors[:,2] → 앵커 width
  - anchors[:,3] → 앵커 height
````python
def encode_boxes(anchors, gt_boxes): 
# 앵커 박스에 대한 실제 바운딩 박스의 오프셋 계산
  # 정답 박스 중심이 앵커 중심에서 width의 몇 배만큼 이동했나
  tx = (gt_boxes[:, 0] - anchors[:, 0]) / anchors[:, 2]
  # 정답 박스 중심이 앵커 중심에서 height의 몇 배만큼 이동했나
  ty = (gt_boxes[:, 1] - anchors[:, 1]) / anchors[:, 3] 
  tw = torch.log(gt_boxes[:, 2] / anchors[:, 2]) 
  th = torch.log(gt_boxes[:, 3] / anchors[:, 3]) 
  return torch.stack([tx, ty, tw, th] , dim=1) # return 값이 offset임

def decode_boxes(anchors, offsets): 
  # 예측된 오프셋을 사용하여 앵커 박스 변환 
  x = offsets[:, 0] * anchors[:, 2] + anchors[:, 0] 
  y = offsets[:, 1] * anchors[:, 3] + anchors[:, 1] 
  w = torch.exp(offsets[:, 2]) * anchors[ , 2]
  h = torch.exp(offsets[:, 3]) * anchors[:, 3] 
  return torch.stack([x, y, w, h] , dim=1)
````

