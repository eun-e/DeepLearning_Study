 ## Chapter 6-1. 순환 신경망의 기초

#### 🔍 개념 정리
- 시계열 데이터: 시간 흐름에 따라 순차적으로 수집된 데이터로, 주식 가격, 음성 신호, 텍스트 등이 해당함 <br>
  └ 관측값이 시간적인 의존성을 가짐
- RNN(순환 신경망): 이전 단계의 정보를 기억하는 순환 구조를 가짐 (은닉층의 뉴런이 자기 자신과 연결) <br>
  <img width="298" height="52" alt="image" src="https://github.com/user-attachments/assets/871c09b3-6f27-45df-9504-ce71a3016019" /> <img width="177" height="56" alt="image" src="https://github.com/user-attachments/assets/49448e73-07d5-4cfc-ab28-74e05b34fc6e" />
  

#### ❓헷갈리는 내용 정리
1. RNN은 왜 기울기 소실과 기울기 폭발이 심각한 문제일까? <br>
   RNN에서 Whh는 시점에 관계없이 항상 고정된 값을 가짐, h(t-1)이랑 h(t)만 변함 <br>
   <img width="205" height="60" alt="image" src="https://github.com/user-attachments/assets/df0b9fad-d047-4ddb-bf19-f79bbc5a0357" /> <br>
   Chain rule에 의해 가중치가 거듭적으로 곱해지기 때문에 W<1이면 기울기 소실, W가 크면 기울기 폭발이 일어남
2. 재현성을 위한 시드 설정 - 특정 숫자를 바탕으로 무적위처럼 보이는 수를 설정<br>
   torch.manual_seed(42): Pytorch 내에서 가중치 초기화 등에 쓰이는 난수를 고정 <br>
   np.random.seed(42): numpy 라이브러리를 사용해 데이터를 섞거나 생성할 때 쓰이는 난수를 고정
<br>

## Chapter 6-2. 고급 순환 신경망 아키텍처

#### 🔍 개념 정리
- LSTM: 순환 신경망의 기울기 소실 문제를 해결하기 위해 제안된 아키텍처 (셀 상태와 게이트 메커니즘)
- 주요 게이트
  1. 망각 게이트: 셀 상태에서 어떤 정보를 버릴지 결정함
  2. 입력 게이트: 새로운 정보 중 어떤 것을 저장할지 결정함
  3. 출력 게이트: 셀 상태에서 어떤 정보를 출력할지 결정
- 기울기 소실 해결: 역전파 시 가중치를 연쇄적으로 곱하는 대신, 덧셈 연산 위주로 정보가 전달되기 때문
<br>

- GRU: LSTM의 간소화 버전<br>
  └ 업데이트 게이트와 리셋 게이트만 사용, 별도의 셀 상태 없이 은닉 상태만 사용
- BRNN: 시퀀스를 정방향과 역방향으로 처리하는 구조<br>
  └ 출력 텐서: (batch_size, sequence_length, hidden_sizeX2) <br>
  └ 최종 은닉 상태 : (num_layersX2, batch_size, hidden_size)<br>
  └ 최종 셀 상태: (num_layersX2, batch_size, hidden_size)
<br>

## Chapter 6-3. 파이토치를 이용한 순환 신경망 구현 (기본)

#### 🔍 개념 정리
- nn.RNN 모듈 사용
- 데이터의 특성과 태스크의 요구 사항을 종합적으로 고려하여 모델 선택해야함

#### ❓헷갈리는 내용 정리
1. Sequence: 순서가 있는 데이터의 나열 (데이터의 길이=시퀀스 길이)
2. 문장은 단어들이 특정 순서로 나열된 시퀀스
3. 가변 길이: 데이터마다 시퀀스 길이가 서로 다른 것   ex) Hi. (길이 1) / I love AI. (길이 3) <br>
   └ 얘네를 같게 맞춰주는게 패딩 or 패킹 기법

## Chapter 6-4. 파이토치를 이용한 순환 신경망 구현 (시계열 데이터)

#### 🔍 개념 정리
- 시계열 데이터 전처리에는 정규화, 윈도우 생성, 데이터 분할이 포함됨
- 윈도우 생성이란? 연속된 시계열을 일정 길이의 작은 조각들로 잘라서 모델이 학습할 수 있는 입력-정답 쌍을 만드는 과정 <br>
  └ 이때 시간 순서는 유지해야함

## Chapter 6-5. 자연어처리 응용

#### 🔍 개념 정리
- 1단계: 텍스트를 토큰화하고 숫자 벡터로 변환하는 것 <br>
  └ 토큰화: 모델이 처리할 수 있는 의미 있는 단위(token)로 쪼개는 과정
- 2단계: 가변 길이 시퀀스 처리를 위해 패딩과 배치 처리를 함
- Attention mechanism을 추가해 중요한 단어에 더 집중할 수 있음


#### ✏️ 추가 정리
- Encoder: 과거 시게열 전체를 읽음, 핵심 패턴을 hidden state로 압축함
- Decoder: encoder가 만든 context를 받아 미래 시계열을 한 스텝씩 생성함





