## Chapter 3-1. 신경망의 개요

#### 🔍 인공뉴런
- 여러 입력값을 가중치와 함께 조합하여 연산을 수행하고, 특정 활성화 함수를 적용해 최종 출력을 생산한다
- 가중치 매개 변수와 편향값을 포함하며, 학습 과정을 통해 조정된다 <br>
  → 가중치는 입력값의 중요도를 결정하며, 학습 과정에서 최적의 값을 찾는다 <br>
  → 편향은 활성화 함수를 거치기 전에 일정한 조정을 수행하여 뉴런의 활성화 여부를 더 정밀하게 조정한다
- 활성화 함수: ReLU, Sigmoid, Tanh 등이 있으며 선형 연산을 비선형 변환한다

#### 🔍 신경망의 종류
1. MLP: 여러 개의 은닉층으로 구성된 완전 연결 신경망 → 공간적, 시간적 구조를 학습하는 능력이 부족해 이미지나 시계열 데이터 처리에는 적합하지 않음
2. CNN: 이미지 데이터와 같은 2차원의 구조화된 데이터에 적합도록 설계된 신경망 → convolution과 pooling 연산을 통해 특징을 추출함
3. RNN: 시계열 데이터를 다루는 데 특화된 신경망 → 이전 상태의 정보를 저장해 시간에 따라 변하는 데이터를 효과적으로 처리함

#### 🔍 신경망의 응용
1. CV: 이미지 및 영상 데이터를 처리해 의미 있는 정보를 추출하는 분야 - CNN 사용
2. NLP: 인간의 언어를 기계가 이해하고 처리하는 기술 - RNN, LSTM, Transformer 사용

<br>

## Chapter 3-2. 기본신경망 구조의 이해

#### 🔍 단일 퍼셉트론의 기본 모델
<img width="250" height="110" alt="image" src="https://github.com/user-attachments/assets/16dc5dbc-7a2e-4f26-8ae3-07ba0f6bb599" /> <br>
- 가중치(w): 초기에는 랜덤 값으로 설정되며, 경사 하강법과 같은 알고리즘을 통해 학습이 진행될수록 업데이트됨 → 과적합되는 경우 가중치 정규화 활용
- 편향: 뉴런이 일정한 출력을 생성할 수 있도록 도움 if 편향이 0인 경우, 활성화 함수가 원점을 중심으로 작동하게 되어 학습 제한됨

#### 🔍 다층 신경망 구조
하나 이상의 은닉층을 포함하며 비선형 문제(XOR)를 해결할 수 있음
- 정규화 기법: L1 및 L2 정규화, Dropout, Batch Normalization

층 깊이를증가하면 신경망의 표현력이 강화되지만, 기울기 소실 문제가 발생 가능함 → Residual Connection 기법 활용
- MLP의 출력층: 분류 문제에서는 Softmax, 회귀 문제에서는 선형 활성화 함수 사용
- MLP의 학습 과정: 손실 함수와 Optimizer(최적화 알고리즘)이 중요한 역할을 하며 Cross-Entropy 손실 함수와 Adam Optimizer가 주로 사용됨

#### 🔍 활성화 함수
- Sigmoid 함수
  1. 출력 범위를 (0,1)로 제한하여 확률적 해석 가능
  2. 기울기 소실 문제 발생 가능
  3. 이진 분류 문제에서 널리 사용됨
     
- ReLU 함수
  1. 입력이 0 이하면 0 출력, 양수일 때는 그대로 출력함
  2. sigmoid보다 학습이 빠르고 기울기 소실 문제 완화 가능
  3. 입력값이 0 이하일 경우는 Dying ReLU 문제 발생 → Leakly ReLU 함수
     
- Tanh 함수
  1. sigmoid와 유사하지만 출력 범위가 (-1,1)로 설정 → 중심이 0인 데이터 학습할 때 유리함
  2. 기울기 소실 문제 발생 가능

- 분류 문제에서는 Softmax, 회귀 문제에서는 Linear Activation function 사용 (은닉층이 아닌 출력층에서 사용)


